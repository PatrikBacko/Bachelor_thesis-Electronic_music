{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'VAE_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     13\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mVAE_1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VAE_1\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'VAE_1'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "import pyaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from VAE_1 import VAE_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_wave(wave, sr):\n",
    "    # initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # open a stream\n",
    "    stream = p.open(format=pyaudio.paFloat32,\n",
    "                    channels=1,\n",
    "                    rate=sr,\n",
    "                    output=True)\n",
    "\n",
    "    # play audio\n",
    "    stream.write(wave.tobytes())\n",
    "\n",
    "    # stop stream and terminate PyAudio\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE_1(\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcuni\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbakalarka\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBachelor_thesis-Electronic_music\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_clap_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\llama\\anaconda3\\envs\\bakalarka\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2103\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copies parameters and buffers from :attr:`state_dict` into\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;124;03mthis module and its descendants. If :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03mthe keys of :attr:`state_dict` must exactly match the keys returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2100\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[1;32m-> 2103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2105\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2106\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."
     ]
    }
   ],
   "source": [
    "model = VAE_1(32)\n",
    "model.load_state_dict(r'C:\\Users\\llama\\Desktop\\cuni\\bakalarka\\Bachelor_thesis-Electronic_music\\data\\models\\model_clap_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, mu, logvar):\n",
    "    model.eval()\n",
    "    z = model.reparameterize(mu, torch.tensor(logvar*2))\n",
    "    with torch.no_grad():\n",
    "        return model.decoder(z)\n",
    "    \n",
    "reconstructed_x = inference(model, torch.zeros((32)), torch.ones((32)))\n",
    "\n",
    "reconstructed_x_np = reconstructed_x[0,0,:,:].numpy()\n",
    "\n",
    "lb.display.specshow(reconstructed_x_np, sr = 44100)\n",
    "\n",
    "inverted = lb.feature.inverse.mfcc_to_audio(reconstructed_x_np, sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "play_wave(inverted, 44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_random_sample(sample_type, paths_to_samples = ):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIEW_SHAPE = 1, 1, 256, 100\n",
    "INVERSE_MFCC_PARAMS = {\n",
    "    'sr': 44100,\n",
    "    'n_mels': 256,\n",
    "    'n_fft': 512,\n",
    "    'hop_length': 256,\n",
    "    'lifter': 0,\n",
    "    'dct_type': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kick = torch.from_numpy(mfccs[0]).view(1, 1, 256,100)\n",
    "\n",
    "\n",
    "print(kick.shape)\n",
    "\n",
    "reconstructed_x, mu, logvar = model.forward(kick)\n",
    "\n",
    "reconstructed_x_np = reconstructed_x[0,0,:,:].detach().numpy()[:,:-2]\n",
    "\n",
    "\n",
    "lb.display.specshow(reconstructed_x_np, sr = 44100)\n",
    "\n",
    "\n",
    "\n",
    "inverted_orig = lb.feature.inverse.mfcc_to_audio(mfccs[0], sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "inverted = lb.feature.inverse.mfcc_to_audio(reconstructed_x_np, sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "\n",
    "play_wave(inverted_orig, 44100)\n",
    "play_wave(inverted, 44100)\n",
    "\n",
    "reconstructed_x = inference(model, mu + 1, logvar + 0.5)\n",
    "reconstructed_x_np = reconstructed_x[0,0,:,:].detach().numpy()[:,:-2]\n",
    "\n",
    "inverted = lb.feature.inverse.mfcc_to_audio(reconstructed_x_np, sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "lb.display.specshow(reconstructed_x_np, sr = 44100)\n",
    "\n",
    "play_wave(inverted, 44100)\n",
    "\n",
    "print(np.mean(mu.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llama\\AppData\\Local\\Temp\\ipykernel_12592\\4034893152.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = model.reparameterize(mu, torch.tensor(logvar*2))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def mfcc_to_wave(mfcc):\n",
    "    wave = lb.feature.inverse.mfcc_to_audio(mfcc, **INVERSE_MFCC_PARAMS)\n",
    "    return wave\n",
    "\n",
    "kick_1_np = mfccs[20]\n",
    "kick_1_torch = torch.from_numpy(kick_1_np).view(*VIEW_SHAPE)\n",
    "kick_1_reconstructed, kick_1_mu, kick_1_logvar = model.forward(kick_1_torch)\n",
    "\n",
    "kick_2_np = mfccs[-13]\n",
    "kick_2_torch = torch.from_numpy(kick_2_np).view(*VIEW_SHAPE)\n",
    "kick_2_reconstructed, kick_2_mu, kick_2_logvar = model.forward(kick_2_torch)\n",
    "\n",
    "interpolated_mu = (kick_1_mu + kick_2_mu) / 2\n",
    "interpolated_logvar = (kick_1_logvar + kick_2_logvar) / 2\n",
    "\n",
    "reconstructed_interpolation = inference(model, interpolated_mu, interpolated_logvar)\n",
    "# The :, :-2 indexing is because of the reconstruction after padding.\n",
    "reconstructed_interpolation_np = reconstructed_interpolation[0, 0, :, :].detach().numpy()[:, :-2]\n",
    "reconstructed_interpolation_wave = mfcc_to_wave(reconstructed_interpolation_np)\n",
    "\n",
    "\n",
    "play_wave(mfcc_to_wave(kick_1_np), INVERSE_MFCC_PARAMS['sr'])\n",
    "play_wave(mfcc_to_wave(kick_2_np), INVERSE_MFCC_PARAMS['sr']) \n",
    "play_wave(reconstructed_interpolation_wave, INVERSE_MFCC_PARAMS['sr'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bakalarka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
