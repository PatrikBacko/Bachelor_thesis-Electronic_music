{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "import pyaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"C:\\Users\\llama\\Desktop\\cuni\\bakalarka\\data\\test\\kicks\"\n",
    "# dir_path = r\"C:\\Users\\llama\\Desktop\\cuni\\bakalarka\\data\\test\\crashes\" \n",
    "# dir_path = r\"C:\\Users\\llama\\Desktop\\cuni\\bakalarka\\data\\drums-one_shots\\kick\\kick_samples\"\n",
    "\n",
    "file_paths = [os.path.join(dir_path, path) for path in os.listdir(dir_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave():\n",
    "    def __init__(self, array, sr, info = None) -> None:\n",
    "        self.array = array\n",
    "        self.sr = sr\n",
    "        self.info = info\n",
    "\n",
    "waves = []\n",
    "\n",
    "for path in file_paths:\n",
    "    array, sr = lb.load(path)\n",
    "    waves.append(Wave(array, sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim(mfcc, length = 100):\n",
    "    if mfcc.shape[1] > length:\n",
    "        return mfcc[:, :length]\n",
    "    else:\n",
    "        last_column = mfcc[:, -1:]\n",
    "        padding = np.repeat(last_column, length - mfcc.shape[1], axis=1)\n",
    "        return np.concatenate((mfcc, padding), axis=1)\n",
    "        # return np.pad(mfcc, ((0,0),(0,length-mfcc.shape[1])), constant_values = mfcc[:,-1])        def pad_or_trim(mfcc, length=100):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = []\n",
    "\n",
    "for wave in waves:\n",
    "    mfcc = lb.feature.mfcc(y=wave.array, sr=wave.sr, n_mfcc=512, n_fft=512, hop_length=256, lifter=0, dct_type=3, n_mels = 256)\n",
    "    mfcc_pad_or_trim = pad_or_trim(mfcc, 100)\n",
    "\n",
    "    mfccs.append(mfcc_pad_or_trim)\n",
    "    # mfccs.append(mfcc)\n",
    "\n",
    "\n",
    "\n",
    "mfccs_tensor = torch.tensor(mfccs).view(-1, 1, 256, 100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mfccs_tensor, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pokus o convolutional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0)\n",
    "        self.fc = nn.Linear(16 * 63 * 24, latent_dim)\n",
    "        self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Encoder input shape: {x.shape}\")\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # print(f\"Encoder shape after conv1: {x.shape}\")\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # print(f\"Encoder shape after conv2: {x.shape}\")\n",
    "        x = nn.Flatten()(x)\n",
    "        # print(f\"Encoder shape after flatten: {x.shape}\")\n",
    "        x = F.relu(self.fc(x))\n",
    "        # print(f\"Encoder shape after fc: {x.shape}\")\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 16 * 63 * 24)\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print()\n",
    "        # print(f\"Dencoder input shape: {x.shape}\")\n",
    "        x = F.relu(self.fc(x))\n",
    "        # print(f\"Decoder shape after fc: {x.shape}\")\n",
    "        x = x.view(-1, 16, 63, 24)\n",
    "        # print(f\"Decoder shape after view: {x.shape}\")\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        # print(f\"Decoder shape after deconv1: {x.shape}\")\n",
    "        x = self.deconv2(x)\n",
    "        # print(f\"Decoder shape after deconv2: {x.shape}\")\n",
    "\n",
    "        #Mirroring of weights with encoder\n",
    "        # F.ConvTranspose(weight = endocer layers ...)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_x = self.decoder(z)\n",
    "        return reconstructed_x, mu, logvar\n",
    "\n",
    "# Define your loss function (e.g., a combination of reconstruction loss and KL divergence)\n",
    "def loss_function(reconstructed_x, x, mu, logvar, kl_regulation = 0.5):\n",
    "    # softmax = nn.Softmax()\n",
    "    # reconstructed_x = softmax(reconstructed_x)\n",
    "    # x = softmax(x)\n",
    "    reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction='sum') #mse for simplicity, could change in the future\n",
    "    kl_divergence = torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (reconstruction_loss - kl_regulation * kl_divergence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM VAE ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, latent_dim):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=256, hidden_size=latent_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(f\"Encoder input shape: {x.shape}\")\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         # print(f\"Encoder shape after conv1: {x.shape}\")\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         # print(f\"Encoder shape after conv2: {x.shape}\")\n",
    "#         x = nn.Flatten()(x)\n",
    "#         # print(f\"Encoder shape after flatten: {x.shape}\")\n",
    "#         x = F.relu(self.fc(x))\n",
    "#         # print(f\"Encoder shape after fc: {x.shape}\")\n",
    "#         mu = self.fc_mu(x)\n",
    "#         logvar = self.fc_logvar(x)\n",
    "#         return mu, logvar\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, latent_dim):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.fc = nn.Linear(latent_dim, 16 * 63 * 24)\n",
    "#         self.deconv1 = nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=0)\n",
    "#         self.deconv2 = nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print()\n",
    "#         # print(f\"Dencoder input shape: {x.shape}\")\n",
    "#         x = F.relu(self.fc(x))\n",
    "#         # print(f\"Decoder shape after fc: {x.shape}\")\n",
    "#         x = x.view(-1, 16, 63, 24)\n",
    "#         # print(f\"Decoder shape after view: {x.shape}\")\n",
    "#         x = F.relu(self.deconv1(x))\n",
    "#         # print(f\"Decoder shape after deconv1: {x.shape}\")\n",
    "#         x = self.deconv2(x)\n",
    "#         # print(f\"Decoder shape after deconv2: {x.shape}\")\n",
    "#         return x\n",
    "\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, latent_dim):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.encoder = Encoder(latent_dim)\n",
    "#         self.decoder = Decoder(latent_dim)\n",
    "\n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         std = torch.exp(0.5 * logvar)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + eps * std\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu, logvar = self.encoder(x)\n",
    "#         z = self.reparameterize(mu, logvar)\n",
    "#         reconstructed_x = self.decoder(z)\n",
    "#         return reconstructed_x, mu, logvar\n",
    "\n",
    "# # Define your loss function (e.g., a combination of reconstruction loss and KL divergence)\n",
    "# def loss_function(reconstructed_x, x, mu, logvar, kl_regulation = 0.5):\n",
    "#     # softmax = nn.Softmax()\n",
    "#     # reconstructed_x = softmax(reconstructed_x)\n",
    "#     # x = softmax(x)\n",
    "#     reconstruction_loss = F.mse_loss(reconstructed_x, x, reduction='sum') #mse for simplicity, could change in the future\n",
    "#     kl_divergence = torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "#     return (reconstruction_loss - kl_regulation * kl_divergence) * (-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "def train(model, train_loader, epochs, device):\n",
    "    #define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, x in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_x, mu, logvar = model(x)\n",
    "            loss = loss_function(reconstructed_x, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        average_loss = train_loss / len(train_loader.dataset)\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch+1, average_loss))\n",
    "        losses.append(average_loss)\n",
    "    print('Finished training.') \n",
    "\n",
    "    return losses\n",
    "\n",
    "# Define your VAE model with a specific latent dimension\n",
    "latent_dim = 32\n",
    "\n",
    "# Create an instance of your VAE model\n",
    "model = VAE(latent_dim)\n",
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "losses = train(model, train_loader, epochs, device)\n",
    "\n",
    "plt.plot(losses)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_wave(wave, sr):\n",
    "    # initialize PyAudio\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # open a stream\n",
    "    stream = p.open(format=pyaudio.paFloat32,\n",
    "                    channels=1,\n",
    "                    rate=sr,\n",
    "                    output=True)\n",
    "\n",
    "    # play audio\n",
    "    stream.write(wave.tobytes())\n",
    "\n",
    "    # stop stream and terminate PyAudio\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, mu, logvar):\n",
    "    model.eval()\n",
    "    z = model.reparameterize(mu, torch.tensor(logvar*2))\n",
    "    with torch.no_grad():\n",
    "        return model.decoder(z)\n",
    "    \n",
    "reconstructed_x = inference(model, torch.zeros((32)), torch.ones((32)))\n",
    "\n",
    "reconstructed_x_np = reconstructed_x[0,0,:,:].numpy()\n",
    "\n",
    "lb.display.specshow(reconstructed_x_np, sr = 44100)\n",
    "\n",
    "inverted = lb.feature.inverse.mfcc_to_audio(reconstructed_x_np, sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "play_wave(inverted, 44100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kick = torch.from_numpy(mfccs[0]).view(1, 1, 256,100)\n",
    "\n",
    "\n",
    "print(kick.shape)\n",
    "\n",
    "reconstructed_x, mu, logvar = model.forward(kick)\n",
    "\n",
    "reconstructed_x_np = reconstructed_x[0,0,:,:].detach().numpy()[:,:-2]\n",
    "\n",
    "\n",
    "lb.display.specshow(reconstructed_x_np, sr = 44100)\n",
    "\n",
    "\n",
    "\n",
    "inverted_orig = lb.feature.inverse.mfcc_to_audio(mfccs[0], sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "inverted = lb.feature.inverse.mfcc_to_audio(reconstructed_x_np, sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "\n",
    "play_wave(inverted_orig, 44100)\n",
    "play_wave(inverted, 44100)\n",
    "\n",
    "reconstructed_x = inference(model, mu + 1, logvar + 0.5)\n",
    "reconstructed_x_np = reconstructed_x[0,0,:,:].detach().numpy()[:,:-2]\n",
    "\n",
    "inverted = lb.feature.inverse.mfcc_to_audio(reconstructed_x_np, sr=44100, n_mels=256, n_fft=512, hop_length=256, lifter=0, dct_type=3)\n",
    "lb.display.specshow(reconstructed_x_np, sr = 44100)\n",
    "\n",
    "play_wave(inverted, 44100)\n",
    "\n",
    "print(np.mean(mu.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\llama\\AppData\\Local\\Temp\\ipykernel_12592\\4034893152.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = model.reparameterize(mu, torch.tensor(logvar*2))\n"
     ]
    }
   ],
   "source": [
    "VIEW_SHAPE = 1, 1, 256, 100\n",
    "INVERSE_MFCC_PARAMS = {\n",
    "    'sr': 44100,\n",
    "    'n_mels': 256,\n",
    "    'n_fft': 512,\n",
    "    'hop_length': 256,\n",
    "    'lifter': 0,\n",
    "    'dct_type': 3\n",
    "}\n",
    "\n",
    "def mfcc_to_wave(mfcc):\n",
    "    wave = lb.feature.inverse.mfcc_to_audio(mfcc, **INVERSE_MFCC_PARAMS)\n",
    "    return wave\n",
    "\n",
    "kick_1_np = mfccs[20]\n",
    "kick_1_torch = torch.from_numpy(kick_1_np).view(*VIEW_SHAPE)\n",
    "kick_1_reconstructed, kick_1_mu, kick_1_logvar = model.forward(kick_1_torch)\n",
    "\n",
    "kick_2_np = mfccs[-13]\n",
    "kick_2_torch = torch.from_numpy(kick_2_np).view(*VIEW_SHAPE)\n",
    "kick_2_reconstructed, kick_2_mu, kick_2_logvar = model.forward(kick_2_torch)\n",
    "\n",
    "interpolated_mu = (kick_1_mu + kick_2_mu) / 2\n",
    "interpolated_logvar = (kick_1_logvar + kick_2_logvar) / 2\n",
    "\n",
    "reconstructed_interpolation = inference(model, interpolated_mu, interpolated_logvar)\n",
    "# The :, :-2 indexing is because of the reconstruction after padding.\n",
    "reconstructed_interpolation_np = reconstructed_interpolation[0, 0, :, :].detach().numpy()[:, :-2]\n",
    "reconstructed_interpolation_wave = mfcc_to_wave(reconstructed_interpolation_np)\n",
    "\n",
    "\n",
    "play_wave(mfcc_to_wave(kick_1_np), INVERSE_MFCC_PARAMS['sr'])\n",
    "play_wave(mfcc_to_wave(kick_2_np), INVERSE_MFCC_PARAMS['sr']) \n",
    "play_wave(reconstructed_interpolation_wave, INVERSE_MFCC_PARAMS['sr'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bakalarka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
